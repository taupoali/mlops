{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Model monitoring\n",
    "\n",
    "In this notebook, you monitor and evaluate the data captured from the endpoint. You create a baseline with which you compare the real-time traffic. Once a baseline is ready, you set up a schedule to continuously evaluate and compare the data against the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1: Environment setup\n",
    "\n",
    "In this task, you set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#install-dependencies\n",
    "%matplotlib inline\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sagemaker import get_execution_role, session\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "from time import sleep\n",
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_session = session.Session(boto3.Session())\n",
    "sm = boto3.Session().client(\"sagemaker\")\n",
    "sm_runtime = boto3.Session().client(\"sagemaker-runtime\")\n",
    "cw = boto3.Session().client(\"cloudwatch\")\n",
    "\n",
    "bucket = sm_session.default_bucket()\n",
    "prefix = 'sagemaker/abalone'\n",
    "data_capture_prefix = \"{}/datacapture\".format(prefix)\n",
    "s3_capture_upload_path = \"s3://{}/{}\".format(bucket, data_capture_prefix)\n",
    "capture_modes = [ \"Input\",  \"Output\" ]\n",
    "code_prefix = \"{}/code\".format(prefix)\n",
    "s3_code_preprocessor_uri = \"s3://{}/{}/{}\".format(bucket, code_prefix, \"preprocessor.py\")\n",
    "s3_code_postprocessor_uri = \"s3://{}/{}/{}\".format(bucket, code_prefix, \"postprocessor.py\")\n",
    "reports_prefix = \"{}/reports\".format(prefix)\n",
    "s3_report_path = \"s3://{}/{}\".format(bucket, reports_prefix)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2: Create a production endpoint with Data Capture enabled\n",
    "\n",
    "To log the inputs to your endpoint and the inference outputs from your deployed model to Amazon S3, you can enable a feature called Data Capture. Data Capture records information that can be used for training, debugging, and monitoring. Amazon SageMaker Model Monitor automatically parses this captured data and compares metrics from this data with a baseline that you create for the model.\n",
    "\n",
    "In this task, you upload the pre-trained model to the S3 bucket, create an Amazon Sagemaker model object, configure an Amazon SageMaker real-time endpoint with Data Capture enabled, and create the real-time endpoint.\n",
    "\n",
    "<i class=\"fas fa-sticky-note\" style=\"color:#ff6633\"></i> **Note:** Endpoint creation takes approximately 5 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create-production-endpoint\n",
    "# Upload models\n",
    "model_url = S3Uploader.upload(\n",
    "    local_path=\"models/model.tar.gz\", desired_s3_uri=f\"s3://{bucket}/{prefix}\"\n",
    ")\n",
    "\n",
    "# Create the model definitions\n",
    "model_name = f\"abalone-A-{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "image_uri = retrieve(\"xgboost\", boto3.Session().region_name, \"1.5-1\")\n",
    "\n",
    "# Create production model object\n",
    "predictor=sm_session.create_model(\n",
    "    name=model_name, role=role, container_defs={\"Image\": image_uri, \"ModelDataUrl\": model_url}\n",
    ")\n",
    "\n",
    "# Create the endpoint configurations\n",
    "variant_name = 'AllTraffic'\n",
    "\n",
    "endpoint_config_name = f'Abalone-Endpoint-1-{datetime.now():%Y-%m-%d-%H-%M-%S}'\n",
    "endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'ModelName':model_name,\n",
    "            'InstanceType':'ml.m5.xlarge',\n",
    "            'InitialInstanceCount':1,\n",
    "            'VariantName':variant_name\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "        DataCaptureConfig= {\n",
    "        'EnableCapture': True, # Whether data should be captured or not.\n",
    "        'InitialSamplingPercentage' : 100,\n",
    "        'CaptureContentTypeHeader': {'CsvContentTypes': [ 'text/csv' ]},\n",
    "        'DestinationS3Uri': s3_capture_upload_path,\n",
    "        'CaptureOptions': [{\"CaptureMode\" : capture_mode} for capture_mode in capture_modes] # Example - Use list comprehension to capture both Input and Output\n",
    "    }\n",
    ")\n",
    "print(f\"Created the Production Model Endpoint Config: {endpoint_config_name}\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Create the endpoint with the production model\n",
    "endpoint_name = f\"Abalone-{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "endpoint_response = sm.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n",
    "\n",
    "def wait_for_endpoint_creation_complete(endpoint):\n",
    "    \"\"\"Helper function to wait for the completion of creating an endpoint\"\"\"\n",
    "    response = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = response.get(\"EndpointStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Endpoint Creation\")\n",
    "        time.sleep(15)\n",
    "        response = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "        status = response.get(\"EndpointStatus\")\n",
    "\n",
    "    if status != \"InService\":\n",
    "        print(f\"Failed to create endpoint, response: {response}\")\n",
    "        failureReason = response.get(\"FailureReason\", \"\")\n",
    "        raise SystemExit(\n",
    "            f\"Failed to create endpoint {endpoint_response['EndpointArn']}, status: {status}, reason: {failureReason}\"\n",
    "        )\n",
    "    print(f\"Endpoint {endpoint_response['EndpointArn']} successfully created.\")\n",
    "\n",
    "wait_for_endpoint_creation_complete(endpoint=endpoint_response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "When the cell completes, an endpoint ARN is returned that looks like *arn:aws:sagemaker:us-west-2:012345678910:endpoint/abalone-2040-10-11-10-11-12*.\n",
    "\n",
    "Your endpoint is currently configured with one variant, the production model. You can view the endpoint configuration using *describe_endpoint*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#describe-the-endpoint\n",
    "sm.describe_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3: View the captured data\n",
    "\n",
    "In this task, you invoke the endpoint created above using production data. Since you already enabled Data Capture on the endpoint, the request payload, response, and additional metadata is saved in the S3 location you specified earlier in the notebook. After invoking the endpoint, you examine the data captured in the S3 bucket.\n",
    "\n",
    "First, use an initialized predictor configured with the endpoint name to invoke the endpoint. Invoking the endpoint with new records helps you confirm your Data Capture configuration is set up correctly. A predictor makes prediction requests to an Amazon SageMaker endpoint. Then, run inference by sending records to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#invoke-the-endpoint\n",
    "predictor = Predictor(endpoint_name=endpoint_name,\n",
    "                        serializer=CSVSerializer(),\n",
    "                        deserializer=CSVDeserializer())\n",
    "\n",
    "validate_dataset = \"abalone_data_new_predictions.csv\"\n",
    "\n",
    "limit = 200  # Need at least 200 samples to compute standard deviations\n",
    "i = 0\n",
    "with open(f\"data/{validate_dataset}\", \"w\") as validation_file:\n",
    "    validation_file.write(\"prediction,label\\n\")  # CSV header\n",
    "    with open(\"data/abalone_data_new.csv\", \"r\") as f:\n",
    "        for row in f:\n",
    "            (label, input_cols) = row.split(\",\", 1)\n",
    "            prediction = predictor.predict(input_cols)[0][0]\n",
    "            validation_file.write(f\"{prediction},{label}\\n\")\n",
    "            i += 1\n",
    "            if i > limit:\n",
    "                break\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            sleep(0.5)\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, list the Data Capture files stored in Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#list-data-capture-files\n",
    "s3_client = boto3.Session().client(\"s3\")\n",
    "current_endpoint_capture_prefix = \"{}/{}\".format(data_capture_prefix, endpoint_name)\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get(\"Contents\")]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, view the contents of a single Data Capture file. You should see all the data captured in an Amazon SageMaker specific JSON-line formatted file. Take a moment to review the first few lines in the captured file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#view-captured-file-lines\n",
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get(\"Body\").read().decode(\"utf-8\")\n",
    "\n",
    "\n",
    "capture_file = get_obj_body(capture_files[-1])\n",
    "print(capture_file[:2000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, view the contents of one captured endpoint input and output record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print-json-file\n",
    "print(json.dumps(json.loads(capture_file.split(\"\\n\")[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enabling Data Capture on your endpoints gives you more flexibility when you want to save information for training, debugging, and monitoring. Since Amazon SageMaker Model Monitor parses this captured data automatically, using Data Capture helps you compare new records to baseline data. \n",
    "\n",
    "You have not configured a baseline yet. In the next task, you use SageMaker Model Monitor to generate baseline statistics and constraints. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.4: Generate baseline statistics and constraints\n",
    "\n",
    "In this task, you create a baseline. Baseline statistics and constraints serve as a standard for detecting data drift and other data quality issues. \n",
    "\n",
    "The test dataset from training the model is often a good baseline dataset. The test dataset schema and the inference dataset schema should exactly match, including the number and order of the features. From the test dataset, you can ask Amazon SageMaker to suggest a set of baseline constraints and generate descriptive statistics to explore the data.\n",
    "\n",
    "First, configure the baseline prefixes and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#configure-baseline-variables\n",
    "baseline_prefix = prefix + \"/baselining\"\n",
    "baseline_data_prefix = baseline_prefix + \"/data\"\n",
    "baseline_results_prefix = baseline_prefix + \"/results\"\n",
    "\n",
    "baseline_data_uri = \"s3://{}/{}\".format(bucket, baseline_data_prefix)\n",
    "baseline_results_uri = \"s3://{}/{}\".format(bucket, baseline_results_prefix)\n",
    "print(\"Baseline data uri: {}\".format(baseline_data_uri))\n",
    "print(\"Baseline results uri: {}\".format(baseline_results_uri))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, start a job to suggest a baseline and constraints. *DefaultModelMonitor.suggest_baseline()* starts a **ProcessingJob** using an Amazon SageMaker provided Model Monitor container to generate the baseline and constraints.\n",
    "\n",
    "<i class=\"fas fa-sticky-note\" style=\"color:#ff6633\"></i> **Note:** A baseline job takes approximately 10 minutes to complete.\n",
    "\n",
    "<i class=\"fas fa-info-circle\" style=\"color:#008296\"></i> **Learn more:** Refer to [Create a Baseline](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-create-baseline.html) for more information about creating baseline calculations of statistics and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create-baselining-job\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.t3.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=\"data/abalone_data_new_withheader.csv\",\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fas fa-sticky-note\" style=\"color:#ec7211\"></i> **NOTE:** This code returns a lengthy response. You can ignore any warnings or error messages.\n",
    "\n",
    "When the cell completes, a message is returned that looks like *2025-10-11 12:13:14,156 - DefaultDataAnalyzer - INFO - Spark job completed*.\n",
    "\n",
    "Now, search for the *constraints.json* and *statistics.json* files to see where they are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#explore-generated-constraints-and-statistics\n",
    "s3_client = boto3.Session().client(\"s3\")\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, view the generated statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#view-statistics\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics table shows each feature with its corresponding summary statistics, including the mean, standard deviation, min, max, and other important details.\n",
    "\n",
    "Finally, view the generated constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#view-constraints\n",
    "constraints_df = pd.json_normalize(\n",
    "    baseline_job.suggested_constraints().body_dict[\"features\"]\n",
    ")\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constraints table shows the inferred type of each feature, the record completeness (in this case 1.0 for all features because the file has no missing values), and the fields that have no non-negative values. \n",
    "\n",
    "Now that you have a baseline created and have viewed the statistics and constraints, create a Model Monitor data quality monitoring job to track new inference records against the baseline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.5: Create a Model Monitor data quality monitoring job\n",
    "\n",
    "After you create your baseline, you can call the *create_monitoring_schedule()* method of the *DefaultModelMonitor* class instance to schedule an hourly data quality monitor.\n",
    "\n",
    "In this task, you analyze and monitor the data with a data quality monitoring job.\n",
    "\n",
    "First, use the *create_monitoring_schedule()* method to schedule an hourly data quality monitoring schedule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create-monitoring-schedule\n",
    "bucket = boto3.Session().resource(\"s3\").Bucket(bucket)\n",
    "bucket.Object(code_prefix + \"/postprocessor.py\").upload_file(\"python/postprocessor.py\")\n",
    "\n",
    "mon_schedule_name = f\"model-monitor-schedule-{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=predictor.endpoint_name,\n",
    "    post_analytics_processor_script=s3_code_postprocessor_uri,\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, send some artificial traffic to the endpoint for the monitoring job to be able to generate the violations report. To simulate data drift, use a set of skewed data. The skewed data, when compared against the baseline, throws an alert with the automated alert triggering system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#send-artificial-traffic\n",
    "endpoint_name = predictor.endpoint_name\n",
    "runtime_client = sm_session.sagemaker_runtime_client\n",
    "limit = 200\n",
    "i = 0\n",
    "\n",
    "# repeating code from above to run this section independently\n",
    "def invoke_endpoint(ep_name, file_name, runtime_client):\n",
    "    i = 0\n",
    "    with open(file_name, \"r\") as f:\n",
    "        for row in f:\n",
    "            (label, payload) = row.strip(\"\\n\").split(\",\", 1)  \n",
    "\n",
    "            response = runtime_client.invoke_endpoint(\n",
    "                EndpointName=ep_name, ContentType=\"text/csv\", Body=payload\n",
    "            )\n",
    "            response[\"Body\"].read()\n",
    "            i += 1\n",
    "            if i > limit:\n",
    "                break\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "\n",
    "invoke_endpoint(endpoint_name, \"data/abalone_data_skewed.csv\", runtime_client)\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use *describe_schedule* to view the schedule you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model-monitor-schedule-status\n",
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "print(\"Schedule status: {}\".format(desc_schedule_result[\"MonitoringScheduleStatus\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The monitor schedule starts jobs at the previously specified hourly interval. Even for an hourly schedule, Amazon SageMaker has a buffer period of 20 minutes to schedule your execution. You might see your execution start anywhere from 0 to 20 minutes from the hour boundary. This is expected and done for load balancing in the backend.\n",
    "\n",
    "This execution takes approximately one hour to be able to generate the violations report. For the purpose of the lab, the next cells have code snippets for you to view and sample output is shared for reference. In the last step of this task, you view the violations report from a file that was generated and pre-loaded from an earlier monitoring run.\n",
    "\n",
    "When the execution finishes, SageMaker reports the status of the latest completed or failed execution. \n",
    "\n",
    "Here are the possible terminal states:\n",
    "- **Completed** - The monitoring execution completed and no issues were found in the violations report. \n",
    "- **CompletedWithViolations** - The execution completed, but constraint violations were detected. \n",
    "- **Failed** - The monitoring execution failed, maybe due to client error (perhaps incorrect role permissions) or infrastructure issues. Further examination of FailureReason and ExitMessage is necessary to identify what exactly happened. \n",
    "- **Stopped** - The job exceeded max runtime or was manually stopped.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to list and view the current status of an execution, you can use code similar to this:\n",
    "\n",
    "```python \n",
    "# list the current execution\n",
    "mon_executions = my_default_monitor.list_executions()\n",
    "print(\n",
    "    \"We created a hourly schedule above that begins executions ON the hour (plus 0-20 min buffer.\\nWe will have to wait for an hour...\"\n",
    ")\n",
    "\n",
    "while len(mon_executions) == 0:\n",
    "    print(\"Waiting for the first execution to happen...\")\n",
    "    time.sleep(60)\n",
    "    mon_executions = my_default_monitor.list_executions()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "# Latest execution status\n",
    "latest_execution = mon_executions[-1]  # Latest execution's index is -1, second to last is -2, etc\n",
    "time.sleep(60)\n",
    "latest_execution.wait(logs=False)\n",
    "\n",
    "print(\"Latest execution status: {}\".format(latest_execution.describe()[\"ProcessingJobStatus\"]))\n",
    "print(\"Latest execution result: {}\".format(latest_execution.describe()[\"ExitMessage\"]))\n",
    "\n",
    "latest_job = latest_execution.describe()\n",
    "if latest_job[\"ProcessingJobStatus\"] != \"Completed\":\n",
    "    print(\n",
    "        \"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\"\n",
    "    )\n",
    "```\n",
    "\n",
    "The following is the expected output when the latest execution of the monitoring job completes.\n",
    "\n",
    "```bash\n",
    "!Latest execution status: Completed\n",
    "\n",
    "Latest execution result: CompletedWithViolations: Job completed successfully with 8 violations.\n",
    "```\n",
    "\n",
    "To list the generated violation report, you can use code similar to this:\n",
    "\n",
    "```python\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "report_uri = latest_execution.output.destination\n",
    "s3uri = urlparse(report_uri)\n",
    "report_bucket = s3uri.netloc\n",
    "report_key = s3uri.path.lstrip(\"/\")\n",
    "s3_client = boto3.Session().client(\"s3\")\n",
    "result = s3_client.list_objects(Bucket=report_bucket, Prefix=report_key)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Report Files:\")\n",
    "print(\"\\n \".join(report_files))\n",
    "```\n",
    "\n",
    "The following is the listing the report files.\n",
    "\n",
    "```bash\n",
    "Found Report Files:\n",
    "sagemaker/abalone/reports/Abalone-2023-09-20-17-05-28/model-monitor-schedule-2023-09-20-17-41-22/2023/09/20/18/constraint_violations.json\n",
    "\n",
    "sagemaker/abalone/reports/Abalone-2023-09-20-17-05-28/model-monitor-schedule-2023-09-20-17-41-22/2023/09/20/18/constraints.json\n",
    "\n",
    "sagemaker/abalone/reports/Abalone-2023-09-20-17-05-28/model-monitor-schedule-2023-09-20-17-41-22/2023/09/20/18/statistics.json\n",
    "```\n",
    "\n",
    "To list violations compare to the baseline, you can use code similar to this:\n",
    "\n",
    "```python\n",
    "violations = my_default_monitor.latest_monitoring_constraint_violations()\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "constraints_df = pd.json_normalize(violations.body_dict[\"violations\"])\n",
    "constraints_df.head(10)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the execution of the monitoring job you started above will not finish for 60-80 minutes, view a violations report from a file that was generated and pre-loaded from an earlier monitoring run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print-violations-report\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "violations = json.load(open('data/violations.json'))\n",
    "constraints_df=pd.json_normalize(violations, record_path=['violations'])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The violations report shows eight violations from the skewed data file, with four **data_type_check** violations for **rings**, **sex_f**, **sex_i**, and **sex_m**, and four **baseline_drift_check** violations for **length**, **diameter**, **whole_weight**, and **shell_weight**.\n",
    "\n",
    "Take a moment to view the description for each violation. Notice that the data type matches were not correct for some of the features. Also, notice that the baseline drift distance exceeded the *0.1* threshold for the four reported features.\n",
    "\n",
    "<i class=\"fas fa-sticky-note\" style=\"color:#ff6633\" aria-hidden=\"true\"></i> **Note:** When you created a baseline job, it produced constraints.json and statistics.json files. In the *constraints.json* file, the *comparison_threshold* is set to *0.1* by default. To learn more about constraints.json file, refer [Schema for Constraints](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-constraints.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.6: Create a CloudWatch alarm\n",
    "\n",
    "When data drift happens, it is helpful to get notifications so you can address any issues. A notification or alarm can also trigger automatic model retraining to address changes that might be occurring with your inference data.\n",
    "\n",
    "In this task, you learn how to create alarm and enable notifications to know when data drifts away from baseline data.\n",
    "\n",
    "First, find your current Amazon Simple Notification Service(Amazon SNS) topics by using **list_topics**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list-topics\n",
    "client = boto3.client('sns')\n",
    "topics_list= client.list_topics()\n",
    "print(topics_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set the **sns_notifications_topic** variable with the topic ARN value you found in the prior cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set-variables\n",
    "topic_details = pd.json_normalize(topics_list['Topics'])\n",
    "topic_arn = topic_details['TopicArn']\n",
    "print (topic_arn[0])\n",
    "sns_notifications_topic = topic_arn[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create an alarm using **put_metric_alarm** that triggers and notification when the feature diameter drifts away from the baseline and retrain the model automatically. \n",
    "\n",
    "You use the built-in Amazon SageMaker Model Monitor container for CloudWatch metrics. SageMaker emits the metrics for each feature observed in the dataset in the */aws/sagemaker/Endpoints/data-metric* namespace with *EndpointName* and *ScheduleName* dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trigger-cloudwatch-alarm-when-it-drifts-from-baseline\n",
    "previous_date = f'{datetime.today() - timedelta(days=1):%Y-%m-%d}'\n",
    "print (previous_date)\n",
    "cw_client = boto3.Session().client('cloudwatch')\n",
    "\n",
    "alarm_name = 'BASELINE_DRIFT_FEATURE_DIAMETER'\n",
    "alarm_desc = 'Trigger an cloudwatch alarm when the feature diameter drifts away from the baseline'\n",
    "feature_diameter_drift_threshold = 0.1  # Setting this threshold purposefully low to see the alarm quickly.\n",
    "metric_name = 'feature_baseline_drift_diameter'\n",
    "namespace = 'aws/sagemaker/Endpoints/data-metrics'\n",
    "\n",
    "endpoint_name = 'Abalone-' + previous_date\n",
    "print (endpoint_name)\n",
    "monitoring_schedule_name = 'model-monitor-schedule-' + previous_date\n",
    "print (monitoring_schedule_name)\n",
    "\n",
    "cw_client.put_metric_alarm(\n",
    "    AlarmName=alarm_name,\n",
    "    AlarmDescription=alarm_desc,\n",
    "    ActionsEnabled=True,\n",
    "    AlarmActions=[sns_notifications_topic],\n",
    "    MetricName=metric_name,\n",
    "    Namespace=namespace,\n",
    "    Statistic='Sum',\n",
    "    Dimensions=[\n",
    "        {\n",
    "            'Name': 'Endpoint',\n",
    "            'Value': endpoint_name\n",
    "        },\n",
    "        {\n",
    "            'Name': 'MonitoringSchedule',\n",
    "            'Value': monitoring_schedule_name\n",
    "        }\n",
    "    ],\n",
    "    Period=600,\n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Threshold=feature_diameter_drift_threshold,\n",
    "    ComparisonOperator='GreaterThanOrEqualToThreshold',\n",
    "    TreatMissingData='breaching'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You created a CloudWatch alarm. You can use this alarm to notify you of any data drift issues and trigger automatic model retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab session and continue with **Task 3**."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
